#Section A
##1 a.

**(i.)** Given an array implmentation of a heap:
```
parent = k/2
left child = k*2
right child = k*2+1
```

**(ii.)** To remove a node at k in a max heap, swap the node with the smallest node in the heap. 

Then 'bubble-down' to reorganize the sub-tree to maintain ordering:

1. Check children of node at k 
2. Swap with the larger of the two children
3. Continue until you have no more children that are greater than your current node

Here is a nice [video tutorial](https://www.youtube.com/watch?v=ijfPvX2qYOQ).

##1 b.

**(i.)** A probe sequence is the order in which a hash function examines a hash table to find a space in which to insert an object. An example is a linear probe sequence which loops through the table one space at a time. See [slide 47](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Data%20Structures.pdf).

**(ii.) Definition of uniform hashing:** given a hash table with *m* slots, a hash function produces *uniform hashing* if, for an unknown key *k*, the probability that the probe sequence of *k* is *p*, where *p* is a permutation of <0, ..., m-1> is the same for all such *p*. See [slide 48](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Data%20Structures.pdf).

Linear probing doesn't produce uniform hashing because by design it fills up the hashtable consecutively and thus indicies close to each other have a higher than random probability of filling up.


**(iii.)** For open-addressing (linear probing), when you delete you only mark the entry as deleted. When inserting you can re-use a deleted entry, but when searching, you cannot stop on a deleted entry. If you do lots of insertions and deletions, then over time you accumulate deleted entries that count against the load factor. Thus performance degrades to O(n), even if the actual load remains low. If you don't delete, open addressing is great.

Read about chained hash tables vs. open addressed tables on [stackoverflow](http://stackoverflow.com/questions/2556142/chained-hash-tables-vs-open-addressed-hash-tables) or see [slides 41-43](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Data%20Structures.pdf) for more information.

Chaining doesn't have this problem as you can insert new values into any node that is not on the linked list (or whatver underlying data structure you're using)

**(iv.)** When using separate chaining for collision resolution in a hash table, the average time required to search/remove an element from the table is O(N/m), that is, it scales linearly with our load factor (N/m, where N is the number of objects and m is the number of slots in the table). Therefore, we don't want our load factor to get too high before we expand the table, or else the time required to search for or remove an element of our hash table will increase.

The logic is similar when we use linear/quadratic probing for collision resolution, though the effects of the load factor are far more dramatic. For each of these, the average number of probes required for an unsuccessful search in our hash table will increase exponentially as our load factor gets close to 1, while increasing rather slowly in approach to a load factor of 0.5.
So, regardless of our means of collision resolution, waiting until we've "filled" our hash table before increasing its size will result in lengthy searches.

[What's the purpose of load factor in hash tables?](https://www.quora.com/Whats-the-purpose-of-load-factor-in-hash-tables)

**(v.)** I would resize it by four times as much as the initial capacity in order avoid exponential search times.

*need to comment on the performance of an insert method*

**Alternative answer** from KS for (iv.) and (v.): Professor Alfaman isn't rehashing his table when he copies it over so he doesn't have simple uniform hashing which means the average running time of search is no longer O(N/m) when he doubles the size of his table. Check out this section of [wikipedia](https://en.wikipedia.org/wiki/Hash_table#Dynamic_resizing) or [slide 43](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Data%20Structures.pdf).

##2 a.

**(i.)** Omega notation is used for asympototic lower bounds so for large values of N the running time will be worse than N-squared.
See [slides 16 and 18](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Introduction%20(1).pdf) or read more from [Khan Academy](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-omega-notation).

**(ii.)** This equivalence is true. The formal definition of O(N) is as follows: O(g(N)) = {f(N) | there are positive constants c and n0 such that 0 <= f(N) <= c*g(N) for all N >= n0}. So in this example g(N) = log(N)/2 and f(N) = log(N)+2. As N approaches infinity, the "+2" will be moot and g(N) will be proportional to half. If g(N) is multiplied by at more than 1/2 (so c > 1/2) then at really large N's the equivalence will hold. See [slide 17](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Introduction%20(1).pdf).

##2 b.

**(i.)** You can sort the array in O(N\*logN) time, and look for duplicates (which would appear beside each other) in O(N) time making the overall time O(N\*logN + N) which simplifies to O(N\*logN). This algorithm has a slightly worse time complexity, but its space complexity is O(1) (the algorithms using hash set has space complexity of O(N), which may be significant).

For more info see [stackoverflow](http://stackoverflow.com/questions/22418681/better-algorithm-with-better-big-o).

**(ii.)** When N is greater than the number of ASCII characters that exist, the array will definitely have repeats so I suppose the time complexity would be O(1) as N gets large. I'm not sure how to answer this question though.

##2 c.

**(i.)** The procedure contains a while loop which runs (up to) k times and the procedure is being run N times so the total amount of time will be proportional to k \* N as N gets large hence O(kN). Not 100% sure this answer is what Tim was looking for though.

```
run INCREMENT N times                 N
procedure INCREMENT(B, k)
  i = 0                               1
  while i < k and B[i] == 1 do        k + 1
    B[i] = 0                          k
    i = i + 1                         k
  if i < k then                       1
    B[i] = 1                          1
    
```

**(ii.)** Our lecture notes don't technically say "accountancy" but if you'd like to read about the various methods, Cornell has some nice [notes](http://www.cs.cornell.edu/courses/cs3110/2011sp/lectures/lec20-amortized/amortized.htm).

N-1, x to increment: | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15... |
-------------------|---|---|---|---|---|---|---|---|---|---|----|----|----|----|----|-------|
bits, k            | 1 | 2 | 2 | 3 | 3 | 3 | 3 | 4 | 4 | 4 | 4  | 4  | 4  | 4  | 4  | 5...  |
flip to zero:      | 0 | 1 | 0 | 2 | 0 | 1 | 0 | 3 | 0 | 1 | 0  | 2  | 0  | 1  | 0  | 4     | 
set bit to one:    | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1  | 1  | 1  | 1  | 1  | 1     |
cost:              | 1 | 2 | 1 | 3 | 1 | 2 | 1 | 4 | 1 | 2 | 1  | 3  | 1  | 2  | 1  | 5     |
avg cost for k:    | 1 | > | 1.5 | > | > | > | 1.75 | > | > | > | >  | >  | >  | >  | 1.875 | 1.9375...  |

">" means "see the value to the right" because otherwise the table doesn't fit on the screen.

It seems that, the more bits there are, the more expensive it becomes (on average) to increment by one but the average cost given k seems to asymptotically approach a limit (sum from 0 to k of 2^(-k) = 2^0 + 2^-1 + 2^-2...). So the amortized cost of 2^(-k) for each N shows that T(N) <= 2^(-k) \* Nc, so T(N) = O(N) hence it could be tightly bound by THETA(N). It's worth mentioning that k is a logarithmic function of N so I'm not sure if that would impact this solution since in a way k is a constant being passed into the function.

##3 a.

##4 a.
  We will use bottom-up approach. Assume P[i] is the amount of money a worker is paid for working i hours, D[i] is the money a worker gets for performing a job of duration i. Note that since we don't have jobs of duration 4 and 6 => D[4] = D[6] = 0
  Hence, 
  ```
    1) P[0] = 0 // worker worked 0 hours and performed 0 jobs
    2) P[1] = max(P[0] + D[1]) = 1 // worker worked just 1 hour and there is only one way he could
                                      spend this time
    3) P[2] = max(P[0] + D[2], P[1] + D[1]) = 6 // choosing between 2 and 6
    4) P[3] = max(P[0] + D[3], P[1] + D[2], P[2] + D[1]) = 8 // choosing between 0+8, 1+6, 6+1
    5) P[4] = max(P[0] + D[4], P[1] + D[3], P[2] + D[2], P[3] + D[1]) = 12 // 2 jobs of duration 2
    6) P[5] = max(P[0] + D[5], P[1] + D[4], P[2] + D[3], P[3] + D[2], P[4] + D[1]) 
            = 16 (P[3] + D[2])
    7) P[6] = max(P[0] + D[6], P[1] + D[5], P[2] + D[4], P[3] + D[3], P[4] + D[2], P[5] + D[1]) 
            = 18 (P[4] + D[2])
    8) P[7] = max(P[0] + D[7], P[1] + D[6], P[2] + D[5], P[3] + D[4], P[4] + D[3], 
                                                         P[5] + D[2], P[6] + D[1]) 
            = 22 (P[5] + D[2])
    9) P[8] = max(P[0] + D[8], P[1] + D[7], P[2] + D[6], P[3] + D[5], P[4] + D[4], 
                                            P[5] + D[3], P[6] + D[2], P[7] + D[1]) 
            = 24 (P[6] + D[2] OR P[5] + D[3])
  ``` 
Note that for the final solutions the combination of jobs is exactly the same, just the order is different.
