#Section A
##1 a.

**(i.)** Given an array implmentation of a heap:
```
parent = k/2
left child = k*2
right child = k*2+1
```

**(ii.)** To remove a node at k in a max heap, swap the node with the smallest node in the heap. 

Then 'bubble-down' to reorganize the sub-tree to maintain ordering:

1. Check children of node at k 
2. Swap with the larger of the two children
3. Continue until you have no more children that are greater than your current node

Here is a nice [video tutorial](https://www.youtube.com/watch?v=ijfPvX2qYOQ).

##1 b.

**(i.)** A probe sequence is the order in which a hash function examines a hash table to find a space in which to insert an object. An example is a linear probe sequence which loops through the table one space at a time.

**(ii.)**

*guesswork*

Linear probing doesn't produce uniform hashing because by design it fills up the hashtable consecutively and thus indicies close to each other have a higher than random probability of filling up.


**(iii.)** For open-addressing (linear probing), when you delete you only mark the entry as deleted. When inserting you can re-use a deleted entry, but when searching, you cannot stop on a deleted entry. If you do lots of insertions and deletions, then over time you accumulate deleted entries that count against the load factor. Thus performance degrades to O(n), even if the actual load remains low. If you don't delete, open addressing is great.

Read about chained hash tables vs. open addressed tables on [stackoverflow](http://stackoverflow.com/questions/2556142/chained-hash-tables-vs-open-addressed-hash-tables).

Chaining doesn't have this problem as you can insert new values into any node that is not on the linked list (or whatver underlying data structure you're using)

**(iv.)** When using separate chaining for collision resolution in a hash table, the time required to search/remove an element from the table is  O(n/m) , that is, it scales linearly with our load factor.

Therefore, we don't want our load factor to get too high before we expand the table, or else the time required to search for or remove an element of our hash table will increase.

The logic is similar when we use linear/quadratic probing for collision resolution, though the effects of the load factor are far more dramatic. For each of these, the average number of probes required for an unsuccessful search in our hash table will increase exponentially as our load factor gets close to 1, while increasing rather slowly in approach to a load factor of 0.5.
So, regardless of our means of collision resolution, waiting until we've "filled" our hash table before increasing its size will result in lengthy searches.

[What's the purpose of load factor in hash tables?](https://www.quora.com/Whats-the-purpose-of-load-factor-in-hash-tables)

v. 

I would resize it by four times as much as the initial capacity in order avoid exponential search times.

*need to comment on the performance of an insert method*

## 2 a.

**(i.)** Omega notation is used for asympototic lower bounds so for large values of N the running time will be worse than N-squared.
See [slides 16 and 18](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Introduction%20(1).pdf) or read more from [Khan Academy](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-omega-notation).

**(ii.)** This equivalence is true. The formal definition of O(N) is as follows: O(g(N)) = {f(N) | there are positive constants c and n0 such that 0 <= f(N) <= c*g(N) for all N >= n0}. So in this example g(N) = ln(N)/2 and f(N) = ln(N)+2. As N approaches infinity, the "+2" will be moot and g(N) will be proportional to half. If g(N) is multiplied by at more than 1/2 (so c > 1/2) then at really large N's the equivalence will hold. See [slide 17](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Introduction%20(1).pdf).

## 2 b.

**(i.)** You can sort the array, and look for duplicates in O(N*logN). This algorithm has a slightly worse time complexity, but its space complexity is O(1) (the algorithms using hash set has space complexity of O(N), which may be significant).

For more info see [stackoverflow](http://stackoverflow.com/questions/22418681/better-algorithm-with-better-big-o)

**(ii.)**














