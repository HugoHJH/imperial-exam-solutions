#Section A
##1 a.

**(i.)** Given an array implmentation of a heap:
```
parent = k/2
left child = k*2
right child = k*2+1
```

**(ii.)** To remove a node at k in a max heap, swap the node with the smallest node in the heap. 

Then 'bubble-down' to reorganize the sub-tree to maintain ordering:

1. Check children of node at k 
2. Swap with the larger of the two children
3. Continue until you have no more children that are greater than your current node

Here is a nice [video tutorial](https://www.youtube.com/watch?v=ijfPvX2qYOQ).

##1 b.

**(i.)** A probe sequence is the order in which a hash function examines a hash table to find a space in which to insert an object. An example is a linear probe sequence which loops through the table one space at a time. See [slide 47](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Data%20Structures.pdf).

**(ii.) Definition of uniform hashing:** given a hash table with *m* slots, a hash function produces *uniform hashing* if, for an unknown key *k*, the probability that the probe sequence of *k* is *p*, where *p* is a permutation of <0, ..., m-1> is the same for all such *p*. See [slide 48](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Data%20Structures.pdf).

Linear probing doesn't produce uniform hashing because by design it fills up the hashtable consecutively and thus indicies close to each other have a higher than random probability of filling up.


**(iii.)** For open-addressing (linear probing), when you delete you only mark the entry as deleted. When inserting you can re-use a deleted entry, but when searching, you cannot stop on a deleted entry. If you do lots of insertions and deletions, then over time you accumulate deleted entries that count against the load factor. Thus performance degrades to O(n), even if the actual load remains low. If you don't delete, open addressing is great.

Read about chained hash tables vs. open addressed tables on [stackoverflow](http://stackoverflow.com/questions/2556142/chained-hash-tables-vs-open-addressed-hash-tables) or see [slides 41-43](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Data%20Structures.pdf) for more information.

Chaining doesn't have this problem as you can insert new values into any node that is not on the linked list (or whatver underlying data structure you're using)

**(iv.)** When using separate chaining for collision resolution in a hash table, the average time required to search/remove an element from the table is O(N/m), that is, it scales linearly with our load factor (N/m, where N is the number of objects and m is the number of slots in the table). Therefore, we don't want our load factor to get too high before we expand the table, or else the time required to search for or remove an element of our hash table will increase.

The logic is similar when we use linear/quadratic probing for collision resolution, though the effects of the load factor are far more dramatic. For each of these, the average number of probes required for an unsuccessful search in our hash table will increase exponentially as our load factor gets close to 1, while increasing rather slowly in approach to a load factor of 0.5.
So, regardless of our means of collision resolution, waiting until we've "filled" our hash table before increasing its size will result in lengthy searches.

[What's the purpose of load factor in hash tables?](https://www.quora.com/Whats-the-purpose-of-load-factor-in-hash-tables)

**(v.)** I would resize it by four times as much as the initial capacity in order avoid exponential search times.

*need to comment on the performance of an insert method*

**Alternative answer** from KS for (iv.) and (v.): Professor Alfaman isn't rehashing his table when he copies it over so he doesn't have simple uniform hashing which means the average running time of search is no longer O(N/m) when he doubles the size of his table. Check out this section of [wikipedia](https://en.wikipedia.org/wiki/Hash_table#Dynamic_resizing) or [slide 43](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Data%20Structures.pdf).

##2 a.

**(i.)** Omega notation is used for asympototic lower bounds so for large values of N the running time will be worse than N-squared.
See [slides 16 and 18](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Introduction%20(1).pdf) or read more from [Khan Academy](https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/big-big-omega-notation).

**(ii.)** This equivalence is true. The formal definition of O(N) is as follows: O(g(N)) = {f(N) | there are positive constants c and n0 such that 0 <= f(N) <= c*g(N) for all N >= n0}. So in this example g(N) = log(N)/2 and f(N) = log(N)+2. As N approaches infinity, the "+2" will be moot and g(N) will be proportional to half. If g(N) is multiplied by at more than 1/2 (so c > 1/2) then at really large N's the equivalence will hold. See [slide 17](https://github.com/timothyylim/imperial-exam-solutions/blob/master/C580-Algorithms/Introduction%20(1).pdf).

##2 b.

**(i.)** You can sort the array in O(N\*logN) time, and look for duplicates (which would appear beside each other) in O(N) time making the overall time O(N\*logN + N) which simplifies to O(N\*logN). This algorithm has a slightly worse time complexity, but its space complexity is O(1) (the algorithms using hash set has space complexity of O(N), which may be significant).

For more info see [stackoverflow](http://stackoverflow.com/questions/22418681/better-algorithm-with-better-big-o).

**(ii.)** I suppose you could use counting sort so you can sort in O(N) time. Then it would be tightly bound by N I think.

##2 c.

**(i.)** The procedure contains a while loop which runs (up to) k times and the procedure is being run N times so the total amount of time will be proportional to k \* N as N gets large hence O(kN). Not 100% sure this answer is what Tim was looking for though.

```
run INCREMENT N times                 N
procedure INCREMENT(B, k)
  i = 0                               1
  while i < k and B[i] == 1 do        k + 1
    B[i] = 0                          k
    i = i + 1                         k
  if i < k then                       1
    B[i] = 1                          1
    
```

**(ii.)** Our lecture notes don't technically say "accountancy" but if you'd like to read about the various methods, Cornell has some nice [notes](http://www.cs.cornell.edu/courses/cs3110/2011sp/lectures/lec20-amortized/amortized.htm).

N-1, x to increment: | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15... |
-------------------|---|---|---|---|---|---|---|---|---|---|----|----|----|----|----|-------|
bits, k            | 1 | 2 | 2 | 3 | 3 | 3 | 3 | 4 | 4 | 4 | 4  | 4  | 4  | 4  | 4  | 5...  |
flip to zero:      | 0 | 1 | 0 | 2 | 0 | 1 | 0 | 3 | 0 | 1 | 0  | 2  | 0  | 1  | 0  | 4     | 
set bit to one:    | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1  | 1  | 1  | 1  | 1  | 1     |
cost:              | 1 | 2 | 1 | 3 | 1 | 2 | 1 | 4 | 1 | 2 | 1  | 3  | 1  | 2  | 1  | 5     |
avg cost for k:    | 1 | > | 1.5 | > | > | > | 1.75 | > | > | > | >  | >  | >  | >  | 1.875 | 1.9375...  |

">" means "see the value to the right" because otherwise the table doesn't fit on the screen.

It seems that, the more bits there are, the more expensive it becomes (on average) to increment by one but the average cost given k seems to asymptotically approach a limit (sum from 0 to k of 2^(-k) = 2^0 + 2^-1 + 2^-2...). So the amortized cost of 2^(-k) for each N shows that T(N) <= 2^(-k) \* Nc, so T(N) = O(N) hence it could be tightly bound by THETA(N). It's worth mentioning that k is a logarithmic function of N so I'm not sure if that would impact this solution since in a way k is a constant being passed into the function.

##3 a.
  inf = infinity
  
  relaxed edges | iteration number | A | B | C | D | E | 
-------------------|---|---|---|---|---|---|
  none            | 0 | 0 | inf | inf | inf | inf | 
(AB)(AC)(AD)      | 1 | 0 | 7(A) | 2(A) | 9(A) | inf | 
(CB)(CD)(CE)    | 2 | 0 | 7(A) | 2(A) | 3(C) | 12(C) | 
(DA)(DE)          | 3 | 0 | 7(A) | 2(A) | 3(C) | 4(D)| 
(EB)    | 4 | 0 | 6(E) | 2(A) | 3(C) | 4(D) | 

### b.
  BFS is correct (return finished vertices in topological order) in terms that it does not need to be reversed.
  DFS firstly goes as deep as possible and only when it reaches the final leaf, it marks it as finished => the vertices will be marked as finished from the least significant (read: the deepest one) to the most significant (read: the starting point). Thus, for every child, its parent will be marked as finished only after the child was marked as finished.
  
  BFS is not suitable for this purpose, because BFS marks each parent "finished" before it finishes with a child => in the reverse order of BFS visited vertices the parent will appear before its child. BFS also returns topologically sorted vertices in order, which does not need to be reversed.
  
  BFS might only be not suitable if the graph contains separate (not connected) trees. 
  
##4 a.
  We will use bottom-up approach. Assume P[i] is the amount of money a worker is paid for working i hours, D[i] is the money a worker gets for performing a job of duration i. Note that since we don't have jobs of duration 4 and 6 => D[4] = D[6] = 0
  Hence, 
  ```
    1) P[0] = 0 // worker worked 0 hours and performed 0 jobs
    2) P[1] = max(P[0] + D[1]) = 1 // worker worked just 1 hour and there is only one way he could
                                      spend this time
    3) P[2] = max(P[0] + D[2], P[1] + D[1]) = 6 // choosing between 2 and 6
    4) P[3] = max(P[0] + D[3], P[1] + D[2], P[2] + D[1]) = 8 // choosing between 0+8, 1+6, 6+1
    5) P[4] = max(P[0] + D[4], P[1] + D[3], P[2] + D[2], P[3] + D[1]) = 12 // 2 jobs of duration 2
    6) P[5] = max(P[0] + D[5], P[1] + D[4], P[2] + D[3], P[3] + D[2], P[4] + D[1]) 
            = 16 (P[3] + D[2])
    7) P[6] = max(P[0] + D[6], P[1] + D[5], P[2] + D[4], P[3] + D[3], P[4] + D[2], P[5] + D[1]) 
            = 18 (P[4] + D[2])
    8) P[7] = max(P[0] + D[7], P[1] + D[6], P[2] + D[5], P[3] + D[4], P[4] + D[3], 
                                                         P[5] + D[2], P[6] + D[1]) 
            = 22 (P[5] + D[2])
    9) P[8] = max(P[0] + D[8], P[1] + D[7], P[2] + D[6], P[3] + D[5], P[4] + D[4], 
                                            P[5] + D[3], P[6] + D[2], P[7] + D[1]) 
            = 24 (P[6] + D[2] OR P[5] + D[3])
  ``` 
Note that for the final solutions the combination of jobs is exactly the same, just the order is different.

### b i).
Top-down
```
  F[0...N] - array to store Fibonacci numbers
  F[0] = 0
  F[1] = 1
  F[2...N] = 0
  //initialise all other numbers to 0
  fib(int n){
    if (n < 2)
      return F[n];
    if (F[n] == 0){
      F[n] = fib(n-2) + fib(n-1) // the fib(n-2) will be calculated before fib(n-1) 
                                    and will speed up the fib(n-1) slightly
    }
    return F[n];
  }
```

### b ii).
Bottom-up

```
  F[0...N] - array to store Fibonacci numbers
  F[0] = 0
  F[1] = 1
  F[2...N] = 0
  //initialise all other numbers to 0
  fib(int n){
    if (i < 2)
      return F[i];
    for i = 2 to n {
      F[i] = F[i-1] + F[i-2]
    }
    return F[i]
  }
```
in dynamic programming Fibonacci only runs O(N) time for bottom-up

### c).
  Dynamic programming cannot really be used within Mergesort except few unlikely corner cases.
  Dynamic programming requires optimal substructure of a problem and overlapping subproblems. Mergesort does have the optimal substructure as sorted sub-arrays are combined into the final arrays, so subproblems do solve a bigger problem. However, mergesort very rarely has overlapping problems as a being-sorted array consists of different elements. Thus, the 2 conditions of DP are not met here. 
  Dynamic programming is only potentially useful if a sorted array consists of repeating sets of elements. E.g.: 
  ```
  fghjk fghjk fghjk fghjk
  ```
  in which case DP will speed up the mergesort proportionally to the amount of repeating sets. Identifying repeating sets in an array can be tricky (time consuming) and bring the benefit of using DP close to 0.
  Other than that, I have no idea what to say here...
### d i).  

```
Pow (x, N){
  if (N == 0)
    return 1;
  if (N == 1)
    return x;
  else if (N % 2 == 1)
    return Pow(x, (N-1)/2) * Pow(x, (N-1)/2) * x;
  else if (N % 2 == 0)
    return Pow(x, N/2) * Pow(x, N/2)
}
```
### d ii).  
  
```
  T(1) = O(1)                             // simply returns x
  T(N) = T(N/2) + T(N/2)                  // for even N
  T(N) = T((N-1)/2) + T((N-1)/2) + T(1)   // for odd N
```
By expanding T(N) even further we will see that T(N) = O(log N)
